#+BLOG: wordpress
#+POSTID: 335
#+DATE: [2016-06-27 Mon 10:32]
#+OPTIONS: toc:nil num:nil todo:nil pri:nil tags:nil ^:nil tex:t  
#+CATEGORY: Cpp, Julia, Computations
#+TAGS:
#+DESCRIPTION:
#+TITLE: Some remarks about min()/max() functions

Computing the *min* (or *max*) of two values looks like a trivial
task.

For *C++* the default Standard Template Library implementation is

#+BEGIN_SRC C++ :exports code
inline const _Tp& min(const _Tp& __a, const _Tp& __b)
{
	return __b < __a ? __b : __a;
}
#+END_SRC

However for numerical computations this definition does not mix well
with the [[https://en.wikipedia.org/wiki/IEEE_floating_point][IEEE 754]] standard when dealing with potential *NaN* value.

For instance, *min()* is *not commutative* and is *not equivatent to
cmath::fmin()*.

We have:

#+BEGIN_SRC C++ :main no :flags -std=c++11 :results org :exports both
#include <iostream>
#include <cmath>
#include <limits>
#include <cassert>

using namespace std;

int main()
{
    const auto x_nan = numeric_limits<double>::quiet_NaN();
    const auto x_1 = 1.;

    cout << boolalpha;

    cout << "\n\ncommutativity?";

    cout << "\n min: "
	 << (min(x_1, x_nan) == min(x_nan, x_1));

    cout << "\nfmin: "
	 << (fmin(x_1, x_nan) == fmin(x_nan, x_1));
}
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
commutativity?
 min: false
fmin: true
#+END_SRC

The [[https://en.wikipedia.org/wiki/NaN][IEEE 754]] says:
#+BEGIN_QUOTE
In section 6.2 of the revised IEEE 754-2008 standard there are two
anomalous functions (the maxnum and minnum functions that return the
maximum of two operands that are expected to be numbers) that favor
numbers â€” *if just one of the operands is a NaN then the value of the
other operand is returned.*
#+END_QUOTE

On the opposite you can read about C++ rules [[http://www.drdobbs.com/cpp/its-hard-to-compare-floating-point-numbe/240149806][here]]:
#+BEGIN_QUOTE
NaN values have the curious property that they compare as "unordered"
with all values, even with themselves. In other words, if x is a NaN,
and y is any floating-point value, NaN or not, then *x<y, x>y, x<=y,
x>=y, and x==y* are *all false*, and *x!=y* is *true*.
#+END_QUOTE

*Remark* In C/C++ that is the reason why you can implement the [[http://en.cppreference.com/w/cpp/numeric/math/isnan][isnan]] function by

#+BEGIN_SRC C++ 
bool isnan(const double x) { return x!=x;}
#+END_SRC

* Using min/max in numerical code

** C++

Like the *IEEE 754* and *C++ standard* do not mix well, I personally
redefine *min()/max()* functions as follow:

#+BEGIN_SRC C++ 
#include <type_traits>
#include <cmath>

namespace libraryNamespace
{
    template <typename T>
    inline enable_if_t<is_floating_point<T>::value, T>
      min(const T& t1, const T& t2)
    {
        return fmin(t1, t2);
    }

    template <typename T>
    constexpr enable_if_t<is_integral<T>::value, const T&>
      min(const T& t1, const T& t2)
    {
        return (t1 < t2) ? t1 : t2;
    }
    // max() function follows the same scheme
}
#+END_SRC

** Julia

Julia follows the scheme I use in C++, you have a specialization for
floating point numbers *julia/base/math.jl*

#+BEGIN_SRC julia
min{T<:AbstractFloat}(x::T, y::T) =
    ifelse((y < x) | (signbit(y) > signbit(x)),
           ifelse(isnan(y), x, y), ifelse(isnan(x), y, x))
#+END_SRC

and a generic operator *julia/base/operator.jl*
#+BEGIN_SRC julia
min(x,y) = ifelse(y < x, y, x)
#+END_SRC

We can check that the *Float* specialization follows the *IEEE 754*
rule:

#+BEGIN_SRC julia :exports both
(max(5,NaN)==max(NaN,5))&&(max(5,NaN)==5)
#+END_SRC

#+RESULTS:
: true



* What is the cost?

** Julia

There is a big difference between the simple definition in
*julia/base/operator.jl* and the *NaN* aware code of
*julia/base/math.jl*. We can have a look at the assembly code:

#+BEGIN_SRC julia :exports code
@code_native(min(Int(5),Int(6)))
#+END_SRC

Gives 
#+BEGIN_EXAMPLE
	.text
	cmpq	%rdi, %rsi
	cmovgeq	%rdi, %rsi
	movq	%rsi, %rax
	retq
	nopl	(%rax,%rax)
	pushq	%rax
	movq	(%rsi), %rax
	movq	(%rax), %rdi
	movq	8(%rsi), %rax
	movq	(%rax), %rsi
	movabsq	$min, %rax
	callq	*%rax
	movabsq	$jl_box_int64, %rcx
	movq	%rax, %rdi
	callq	*%rcx
	popq	%rcx
	retq
#+END_EXAMPLE

whereas 

#+BEGIN_SRC julia :exports code
@code_native(min(Float64(5),Float64(6)))
#+END_SRC

Gives 
#+BEGIN_EXAMPLE
L66:	.text
	ucomisd	%xmm1, %xmm0
	seta	%al
	movd	%xmm1, %rcx
	cmpq	$0, %rcx
	setl	%cl
	movd	%xmm0, %rdx
	cmpq	$-1, %rdx
	setg	%dl
	andb	%cl, %dl
	orb	%al, %dl
	testb	$1, %dl
	jne	L66
	movapd	%xmm0, %xmm2
	cmpordsd	%xmm2, %xmm2
	andpd	%xmm2, %xmm0
	andnpd	%xmm1, %xmm2
	orpd	%xmm0, %xmm2
	movapd	%xmm2, %xmm0
	retq
	movapd	%xmm1, %xmm2
	cmpordsd	%xmm2, %xmm2
	andpd	%xmm2, %xmm1
	andnpd	%xmm0, %xmm2
	orpd	%xmm1, %xmm2
	movapd	%xmm2, %xmm0
	retq
	nopl	(%rax)
	pushq	%rax
	movq	(%rsi), %rax
	movsd	(%rax), %xmm0           # xmm0 = mem[0],zero
	movq	8(%rsi), %rax
	movsd	(%rax), %xmm1           # xmm1 = mem[0],zero
	movabsq	$min, %rax
	callq	*%rax
	movsd	%xmm0, (%rsp)
	movabsq	$jl_gc_alloc_1w, %rax
	callq	*%rax
	movabsq	$140364146520656, %rcx  # imm = 0x7FA91317B250
	movq	%rcx, -8(%rax)
	movsd	(%rsp), %xmm0           # xmm0 = mem[0],zero
	movsd	%xmm0, (%rax)
	popq	%rcx
	retq
#+END_EXAMPLE

** C++

We have the same in C++

#+BEGIN_SRC C++ :exports code
#include <cmath>
#include <iostream>

int main()
{
  double x, y;
  std::cin >> x >> y;
  asm("#ASM FOR FMIN");
  double fmin_x_y = std::fmin(x, y);
  asm("#ASM FOR FMIN - END");
  std::cout << "\n" << fmin_x_y;

  asm("#ASM FOR MIN");
  double min_x_y = std::min(x, y);
  asm("#ASM FOR MIN - END");
  std::cout << "\n" << min_x_y;
  return 0;
}
#+END_SRC

compiled with 
#+BEGIN_EXAMPLE
g++ -std=c++11 -O3 -S min.cpp -o min.asm
#+END_EXAMPLE

gives for *min()*
#+BEGIN_EXAMPLE
#ASM FOR MIN
	movsd	24(%rsp), %xmm0
	minsd	16(%rsp), %xmm0
	movsd	%xmm0, 8(%rsp)
#ASM FOR MIN - END
#+END_EXAMPLE

and for *fmin()*

#+BEGIN_EXAMPLE
#ASM FOR FMIN
	movsd	24(%rsp), %xmm1
	movsd	16(%rsp), %xmm0
	call	fmin
	movsd	%xmm0, 8(%rsp)
#ASM FOR FMIN - END
#+END_EXAMPLE

* CPU time?

Illustration with the [[http://theory.stanford.edu/~sergei/papers/soda10-jaccard.pdf][Jaccard distance]] defined by
$$
d_J(x,y)=1-\frac{\sum\limits_i \min(x_i,y_i)}{\sum\limits_i \max(x_i,y_i)},\ \text{where}\ (x,y)\in\mathbb{R}_+^n\times\mathbb{R}_+^n
$$

We compute this distance using *4 different approaches*:
- Julia min()/max() NaN aware
- Julia min()/max() comparison
- C fmin()/fmax() NaN aware
- C min()/max() comparison

The Julia code is given below

# To generate: C-c C-v t

#+BEGIN_SRC julia :tangle yes :tangle jaccard.jl
function jaccard_julia_NaN_aware(a::Array{Float64,1},
                                 b::Array{Float64,1})

    @assert length(a)==length(b) 

    num::Float64 = 0
    den::Float64 = 0

    for i in 1:length(a)

        @inbounds num += min(a[i],b[i])
        @inbounds den += max(a[i],b[i])

    end
    return 1. - num/den
end

function jaccard_julia_comparison(a::Array{Float64,1},
                                  b::Array{Float64,1})

    @assert length(a)==length(b) 

    num::Float64 = 0
    den::Float64 = 0

    for i in 1:length(a)

        @inbounds num += ifelse(a[i]<b[i],a[i],b[i])
        @inbounds den += ifelse(a[i]>b[i],a[i],b[i])

    end
    return 1. - num/den
end

function jaccard_C_NaN_aware(a::Array{Float64,1},
                             b::Array{Float64,1})

    @assert length(a)==length(b) 

    return ccall((:jaccard_C_NaN_aware,
                  "./libjaccard.so"),
                 Float64,
                 (Int64,Ptr{Float64},Ptr{Float64}),
                 length(a),a,b)

end

function jaccard_C_comparison(a::Array{Float64,1},
                              b::Array{Float64,1})

    @assert length(a)==length(b) 

    return ccall((:jaccard_C_comparison,
                  "./libjaccard.so"),
                 Float64,
                 (Int64,Ptr{Float64},Ptr{Float64}),
                 length(a),a,b)

end

function test_distance(f,
                       v1::Array{Float64,1},
                       v2::Array{Float64,1})
    sum=0
    for i in 1:5000
        sum+=f(v1,v2)
    end
    sum
end

v1=rand(10000);
v2=rand(10000);

for name in (:jaccard_julia_NaN_aware,
             :jaccard_C_NaN_aware,
             :jaccard_julia_comparison,
             :jaccard_C_comparison)
    print("$name")
    @time @eval test_distance($name,v1,v2)
end
#+END_SRC

The associated *C* subroutines are:

#+BEGIN_SRC C :tangle yes :main no :tangle jaccard.c
#include <math.h>
#include <stdint.h>

double jaccard_C_NaN_aware(uint64_t size,
			   double *a, double *b)
{
  double num = 0, den = 0;

  for(uint64_t i = 0; i < size; ++i)
    {
      num += fmin(a[i],b[i]);
      den += fmax(a[i],b[i]);
    }
  return 1. - num / den;
}

double jaccard_C_comparison(uint64_t size,
			    double *a, double *b)
{
  double num = 0, den = 0;

  for(uint64_t i = 0; i < size; ++i)
    {
      num += (a[i] < b[i] ? a[i] : b[i]);
      den += (a[i] > b[i] ? a[i] : b[i]);
    }
  return 1. - num / den;
}
#+END_SRC

and the *Makefile* is

#+BEGIN_SRC sh :tangle yes :tangle Makefile
all: bench

bench: libjaccard.so
	@julia --optimize --check-bounds=no jaccard.jl

libjaccard.so: jaccard.c
	@gcc -O2 -shared -fPIC jaccard.c -o libjaccard.so
#+END_SRC

#+RESULTS:

The results I get on my computer are:
#+BEGIN_SRC sh :exports code
make
#+END_SRC

#+tblname: bench-table
#+RESULTS:
| jaccard_julia_NaN_aware  | 0.634061 | seconds | (25.57 | k | allocations: | 811.886 | KB) |
| jaccard_C_NaN_aware      | 0.404112 | seconds | (12.26 | k | allocations: | 262.989 | KB) |
| jaccard_julia_comparison | 0.206744 | seconds | (21.36 | k | allocations: | 601.927 | KB) |
| jaccard_C_comparison     | 0.084586 | seconds | (12.24 | k | allocations: | 262.088 | KB) |

#+BEGIN_SRC gnuplot :var data=bench-table :file output.png :exports results
reset
set xtics ("NaN aware" 0, "Comparison" 1)
set ylabel "CPU time (seconds)"

set boxwidth 0.5
set style fill solid

plot data every 2    using 2 with boxes ls 1 title "Julia",\
     data every 2::1 using 2 with boxes ls 4 title "C"
#+END_SRC

#+RESULTS:
[[file:output.png]]


# output.png http://pixorblog.files.wordpress.com/2016/07/output.png

We see that:
- *C* is still faster than *Julia*
- min()/max() using simple comparisons is more than *2 times* faster than an implementation
  taking into account possible *NaN* values.

* A source of bugs?

You have to take care that a simple statement like $$ x\le\min(x,y) $$
is mathematically true but false in your code. It is even false for
both the IEEE 754 and the comparison based versions of the *min()* function.

In the future I will write a post on [[https://en.wikipedia.org/wiki/Automatic_differentiation][Automatic Differentiation]]. To be
brief *automatic differentiation* is a tool that allows you to
*efficiently* compute gradient with nearly no modification of your
original code. As example my personal implementation takes the form:

#+BEGIN_SRC C++
// Declare the current tape
//
AD_Tape<double> tape;

// Computes gradient of f(x,y,z)=x.z.sin(x.y)
//                   at (x,y,z)=(2,1,5)
//
// Note:
//
// grad(f)={ x * y * z * Cos(x * y) + z * Sin(x * y),
//           x^2 * z *
//           Cos(x * y), x * Sin(x * y) }
//
AD_Scalar<double> x, y, z, f;

x = 2;
y = 1;
z = 5;

f = x * z * sin(x * y);

const auto grad = ad_gradient(f);

std::cout << "\ngrad(f) = {"
          << grad[x] << "," 
          << grad[y] << ","
          << grad[z] << "}";

// The screen output is
//
// grad(f) = {0.385019,-8.32294,1.81859}
#+END_SRC

One classical approach uses *operator overloading*. For each basic
operation you compute the *function value* and its *differential*.

One *important and desirable property of AutoDiff library* is that its
use *does not modify your program result.*

Unfortunately a lot of AutoDiff libraries are bugged when they define
the min()/max() functions.

It is really "natural" to define *min()* overloading by something like:

#+BEGIN_SRC julia
function min(x,y)
    ifelse(x<y,
           return (x,dx),
           return (y,dy))
#+END_SRC

But *this implementation is buggy:* if *y* is *NaN* we now know that *x<y* is always false, hence:
- the original code will return *x*
- the AutoDiff code will return *(y,dy)* (a priori *=(NaN,NaN)*)

To stay *consistent* the *correct* implementation is something like:

#+BEGIN_SRC julia
function min(x,y)
    ifelse(x==min(x,y),
           return (x,dx),
           return (y,dy))
#+END_SRC

which provides the right result (id consistent with the original code)
whatever *x* or *y* is *NaN* or not

We can check that, for instance, with [[https://github.com/JuliaDiff/ForwardDiff.jl][Julia ForwardDiff.jl]] 

(githash: 045a828)

#+BEGIN_SRC julia :results output :exports both
using ForwardDiff
f(x::Vector)=max(2*x[1],x[2]);
x=[1.,NaN]
print("Original value: $(f(x))")
print("\nAutoDiff gradient: $(ForwardDiff.gradient(f, x))")
#+END_SRC

#+RESULTS:
: Original code: 1.0
: AutoDiff code: [0.0,1.0]

which is *wrong*, in this case the *right* gradient is
$$
\nabla [(x_1,x_2)\rightarrow max(2.x_1,x_2)]_{x_1=1,x_2=NaN}=(2,0)
$$

* Final word

If you are fed up with min()/max() you can use *absolute value*:
$$
min(x,y)=\frac{1}{2}(|x+y|-|x-y|)
$$
$$
max(x,y)=\frac{1}{2}(|x+y|+|x-y|)
$$

... but these relations do not follow the IEEE 754 standard!


I would like to thank my colleague JP. Both for having noticed that
min()/max() was "abnormally" slow and Y. Borstein for some emails
exchanges concerning min()/max() in Julia.
